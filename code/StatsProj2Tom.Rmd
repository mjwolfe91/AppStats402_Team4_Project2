---
author: "Tom Gianelle"
date: "8/05/2019"
output: 
  html_document:
    keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(readxl)
library(tidyquant)
library(pastecs)
library(ggplot2)
library(corrplot)
library(MASS)
library(rpart.plot)
library(ROSE)
library(randomForest)
library(caret)
library(RCurl)
library(ROCR)
library(glmnet)

```

``` {r SessionInfo}
sessionInfo()
```

### Objective 2:  With a simple logistic regression model as a baseline, perform additional competing models to improve on prediction performance metrics.  Which metrics are up to you and your given data set.
### •	Record the predictive performance metrics from your simple, highly interpretable model from Objective 1.
### •	You must include one additional logistic regression model which is also a more complicated logistic regression model than in Objective 1.  By complicated, I do not mean that you include more predictors (that will be somewhat sorted out in Objective 1), but rather model complexity through interaction terms, new variables created by the group, transformations or additions through polynomials.

### •	Create another competing model using just the continuous predictors and use LDA or QDA.  

### •	Use a nonparameteric model approach as a competing model.  Random forest or decision tree for predictors that are both categorical and continuous or a k-nearest neighbors approach if just working with continuous predictors. 

### •	Provide a summary table of the performance across the competing methods. Summarize the overall findings.  A really great report will also give insight as to why the “best” model won out.  This is where a thorough EDA will always help.

## Load Data

```{r}
heartURL <- getURL("https://raw.githubusercontent.com/mjwolfe91/AppStats402_Team4_Project2/master/data/framingham.csv")
heartData <- read.csv(text=heartURL, header=TRUE)
#setwd("~/Documents/SMU/_Applied Statistics/Project 2 ")
#heartData <- read.csv(file = "framingham.csv", header=TRUE)
#heartData &lt;- read.csv('C:/Users/micha/OneDrive/Desktop/SMU MSDS/Applied Stats/Project 2/Data/framingham.csv')
head(heartData)
```

Some of these data have categories that aren't particularly intuitive. Let's recode some of them.

## Rename columns

```{r recode}
#https://dplyr.tidyverse.org/reference/recode.html

heartData$male = recode(heartData$male, '1' = 'male', '0' = 'female')
names(heartData)[1] = 'Gender'
unique(heartData$Gender)

heartData$education = recode(heartData$education, '1'='SomeHighSchool', '2'='FinishedHighSchool/GED',
       '3'='SomeCollege/VocationalSchool', '4'='College')
unique(heartData$education)

heartData$currentSmoker = recode(heartData$currentSmoker, '1'='Yes', '0'='No')
unique(heartData$currentSmoker)

heartData$BPMeds = recode(heartData$BPMeds, '1'='Yes', '0'='No')
unique(heartData$BPMeds)

unique(heartData$prevalentStroke)
heartData$prevalentStroke = recode(heartData$prevalentStroke, '1'='Yes', '0'='No')

unique(heartData$prevalentHyp)
heartData$prevalentHyp = recode(heartData$prevalentHyp, '1'='Yes', '0'='No')

unique(heartData$TenYearCHD)
heartData$TenYearCHD = recode(heartData$TenYearCHD, '1'='Yes', '0'='No')

unique(heartData$diabetes)
heartData$diabetes = recode(heartData$diabetes, '1'='Yes', '0'='No')
```

## EDA

## Column analysis and cleanup

Let's first confirm that our datatypes agree with the analysis we want to do.

```{r pairs}
sapply(heartData, class)
```

Those character columns would do better as factors, so let's change them.

```{r pairs}
cols_factor <- c("Gender", "education", "currentSmoker", "BPMeds", "prevalentStroke", "prevalentHyp", "diabetes", "TenYearCHD")
heartData[cols_factor] &lt;- lapply(heartData[cols_factor], factor)
```

Now let's take some sums and summaries.

```{r}
numeric_cols <- subset(heartData, select = -c(Gender, education, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes, TenYearCHD))
colSums(numeric_cols)
colSums(is.na(heartData))
summary(heartData)
```

Looks like there's some missing values in cigsPerDay, totChol, heartRate, and glucose. Let's see how much leverage these values have.

```{r}
dim(heartData)
dim(na.omit(heartData))
3658/4240
```

We can keep 86% of our values, so let's go ahead and remove these rows.

```{r}
heartData = na.omit(heartData)
dim(heartData)
summary(heartData)
```

Much better! However, it would be easier to interpret the model outcome as a person with _ is _ more likely to be diagnosed with heart disease, so let's adjust the response variable accordingly.

```{r}
levels(heartData$TenYearCHD)
heartData$TenYearCHD = relevel(heartData$TenYearCHD, ref='No')
```

## Continuous variables

Let's start by checking correlations.

```{r glucose}
aggregate(glucose~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=glucose)) + geom_boxplot()
```

Lots of outliers here. Hard to say if there is correlation because of this.

```{r heartrate}
aggregate(heartRate~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=heartRate)) + geom_boxplot()
```

No correlation here.

```{r BMI}
aggregate(BMI~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=BMI)) + geom_boxplot()
```

Looks like high BMI and heart disease might be correlated.

```{r diastolic BP}
aggregate(diaBP~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=diaBP)) + geom_boxplot()
```

Similarly appears that high diastolic BP correlates to ten year CHD.

```{r systolic BP}
aggregate(sysBP~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=sysBP)) + geom_boxplot()
```

Same with systolic BP.

```{r total cholesterol}
aggregate(totChol~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=totChol)) + geom_boxplot()
```

Again with cholesterol.

```{r cigarettes per day}
aggregate(cigsPerDay~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=cigsPerDay)) + geom_boxplot()
```

Yet again with cigarette use. Lots of outliers here too.

```{r age}
aggregate(age~TenYearCHD, data=heartData, summary)
ggplot(heartData, aes(x=TenYearCHD, y=age)) + geom_boxplot()
```

Higher age appears to be associated with CHD as well.

Let's run some histograms.

```{r histograms}
for (i in colnames(numeric_cols)){
  hist(heartData[[i]])
}
```

Mostly normally distributed with a couple exceptions. Now let's check mutlicolinearity and correlations.

```{r pairs correlation}
pairs(~ age + cigsPerDay + totChol + sysBP + diaBP + BMI + heartRate + glucose, data=heartData)
pairs(~ age + cigsPerDay + totChol + sysBP + diaBP + BMI + heartRate + glucose, data=heartData,
      col=heartData$TenYearCHD)
heartData.cor &lt;- heartData %&gt;% select(age, cigsPerDay, totChol, sysBP, diaBP, BMI, heartRate, glucose) %&gt;%
  cor()
heartData.cor
heatmap.2(heartData.cor,col=redgreen(20), 
          density.info="none", trace="none", dendrogram=c("row"), 
          symm=F,symkey=T,symbreaks=T, scale="none")
```

There's a strong correlation between systolic blood pressure and diastolic blood pressure...which makes practical sense considering they are essentially the same measure. Age does a good job of stratifying the variables and reducing multicollinearity. Two takeaways from this - first might be to remove either systolic or diastolic BP, second is to perhaps create interactions on the age variable.

## Categorical variables

Next let's check the categorical variables, first by measuring and plotting proportionality.

```{r diabetes}
attach(heartData)
prop.table(table(TenYearCHD,diabetes),2)
plot(TenYearCHD~diabetes,col=c("red","blue"))
```

There appears to be an unbalanced response rate. We will need to keep this in mind when creating training data, and will likely necessitate a correction to the coefficient. It also appears that prevalent diabetes is associated with ten year CHD.

```{r prevalentHyp}
prop.table(table(TenYearCHD,prevalentHyp),2)
plot(TenYearCHD~prevalentHyp,col=c("red","blue"))
```

Again we see prevalent hypertension associated with ten year CHD.

```{r prevalentStroke}
prop.table(table(TenYearCHD,prevalentStroke),2)
plot(TenYearCHD~prevalentStroke,col=c("red","blue"))
```

Another association

```{r BPMeds}
prop.table(table(TenYearCHD,BPMeds),2)
plot(TenYearCHD~BPMeds,col=c("red","blue"))
```

Another association, and class imbalance

```{r currentSmoker}
prop.table(table(TenYearCHD,currentSmoker),2)
plot(TenYearCHD~currentSmoker,col=c("red","blue"))
```

Oddly appears that there is no association between smoking and ten year CHD

```{r education}
prop.table(table(TenYearCHD,education),2)
mosaicplot(TenYearCHD~education, las=1, main = 'Education vs Heart Disease')
```

Higher education appears to have lower rates of ten year CHD.

```{r gender}
prop.table(table(TenYearCHD,currentSmoker),2)
plot(TenYearCHD~Gender,col=c("red","blue"), las=1)
```

Males appear to develop CHD more often.

```{r ftables}
ftable(addmargins(table(heartData$TenYearCHD,heartData$currentSmoker))) 
ftable(addmargins(table(heartData$TenYearCHD,heartData$Gender))) 
ftable(addmargins(table(heartData$TenYearCHD,heartData$education))) 
ftable(addmargins(table(heartData$TenYearCHD,heartData$BPMeds))) 
ftable(addmargins(table(heartData$TenYearCHD,heartData$prevalentStroke))) 
ftable(addmargins(table(heartData$TenYearCHD,heartData$prevalentHyp))) 
ftable(addmargins(table(heartData$TenYearCHD,heartData$diabetes)))
```
## --------
## Tom - More complex model with interactions, LDA and RF
## --------


## Correlations
library(corrplot)
correlations <- cor(heartData)
corrplot(correlations, method="circle")

## Complex logistic regression  ---  using LASSO and Forward model -- tested numerous interactions.   One example below.   None resulted in a better model.
#Training Set
dat.train <- heartData
dat.train.x <- dat.train[,1:15]

dat.train.y <- dat.train$TenYearCHD
dat.train.y <- as.factor(as.character(dat.train.y))

glm.fit <- glm(dat.train.y ~ age+sysBP+cigsPerDay+male+diabetes+prevalentStroke+age:sysBP+age:diabetes+sysBP:diabetes, data = heartData, family = binomial)

summary(glm.fit)

## LDA  --- all the variables

#Training Set
dat.train <- heartData[1:3000,]
dat.train.x <- dat.train[,1:15]

dat.train.y <- dat.train$TenYearCHD
dat.train.y <- as.factor(as.character(dat.train.y))

fit.lda <- lda(dat.train.y ~ ., data = dat.train.x)
pred.lda <- predict(fit.lda, newdata = dat.train.x)

preds <- pred.lda$posterior
preds <- as.data.frame(preds)

pred <- prediction(preds[,2],dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


## LDA with only continous variables

#Training Set
dat.train <- heartData[1:3000,]
dat.train.x <- dat.train[,1:15]

dat.train.y <- dat.train$TenYearCHD
dat.train.y <- as.factor(as.character(dat.train.y))

fit.lda <- lda(dat.train.y ~ age+cigsPerDay+totChol+sysBP+BMI+heartRate+glucose, data = dat.train.x)
pred.lda <- predict(fit.lda, newdata = dat.train.x)

preds <- pred.lda$posterior
preds <- as.data.frame(preds)

pred <- prediction(preds[,2],dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


## LDA  --- with combined LASSO and Forward model

#Training Set
dat.train <- heartData[1:3000,]
dat.train.x <- dat.train[,1:15]

dat.train.y <- dat.train$TenYearCHD
dat.train.y <- as.factor(as.character(dat.train.y))

fit.lda <- lda(dat.train.y ~ age+sysBP+cigsPerDay+male+diabetes+prevalentStroke, data = dat.train.x)
pred.lda <- predict(fit.lda, newdata = dat.train.x)

preds <- pred.lda$posterior
preds <- as.data.frame(preds)

pred <- prediction(preds[,2],dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))



### Decision Tree

term_vars <- c("male","age", "education","currentSmoker", "cigsPerDay", "BPMeds", "prevalentStroke", "prevalentHyp", "diabetes", "totChol", 	"sysBP",	"diaBP", 	"BMI", "heartRate", "glucose", "TenYearCHD")
#term_vars <- c("male","age", "currentSmoker", "prevalentStroke", "prevalentHyp", "diabetes", 	"sysBP",	"diaBP", "education",	"cigsPerDay", "BPMeds", "totChol", "BMI", "heartRate", "glucose", "TenYearCHD")
CHD_term_train <- heartData[1:3000,]
CHD_term_test <- heartData[3001:nrow(heartData),]
set.seed(99)  # set a pre-defined value for the random seed so that results are repeatable
# Decision tree model
rpart_model <- rpart(TenYearCHD ~.,
                     data = CHD_term_train[term_vars],
                     method = 'class',
                     parms = list(split='information'),
                     control = rpart.control(usesurrogate = 0,
                                             maxsurrogate = 0))
# Plot the decision tree
rpart.plot(rpart_model, roundint = FALSE, type = 3)



#Random Forest

dat.train.rf <- heartData[1:3000,]
dat.train.rf$TenYearCHD<-factor(dat.train.rf$TenYearCHD)

CHD_train_rose <- ROSE(TenYearCHD ~ ., data = dat.train.rf, seed=125)$data

train.rf<-randomForest(TenYearCHD~.,data=CHD_train_rose,ntree=500,importance=T,na.action = na.omit)
fit.pred<-predict(train.rf,newdata=dat.train.rf,type="prob")



pred <- prediction(fit.pred[,2], dat.train.rf$TenYearCHD)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))








#Predict Validation Set I --- use last 3rd of heartData data


dat.val1.rf <- heartData[3001:nrow(heartData),]

pred.val1<-predict(train.rf,newdata=dat.val1.rf,type="prob")



pred <- prediction(pred.val1[,2], dat.val1.rf$TenYearCHD)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))





#Multiple random sample data validation checks
setwd("~/Documents/SMU/_Applied Statistics/Project 2 ")
heartData <- read.csv(file = "framingham.csv", header=TRUE)
head(heartData)

dat.train <- heartData[1:3000,]
dat.train.x <- dat.train[,1:15]

dat.train.y <- dat.train$TenYearCHD
dat.train.y <- as.factor(as.character(dat.train.y))

nloops<-50   #number of CV loops
#CHD_cont_df <- data.frame(heartData[1], heartData[10:15])
ntrains<-dim(dat.train.x)[1]  #No. of samples in training data set
cv.aucs<-c() #initializing a vector to store the auc results for each CV run
#dat.train.x <- data.frame(heartData[1], heartData[10:15])



for (i in 1:nloops){
 index<-sample(1:ntrains,1000)
 cvtrain.x<-as.matrix(dat.train.x[index,])
 cvtest.x<-as.matrix(dat.train.x[-index,])
 cvtrain.y<-dat.train.y[index]
 cvtest.y<-dat.train.y[-index]
 
 train <- data.frame(cvtrain.x, cvtrain.y)
 
 myFolds <- createFolds(cvtrain.y, k = 5)

 myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE, 
  verboseIter = TRUE,
  savePredictions = TRUE,
  index = myFolds
 )
 cvfit <- train(cvtrain.y ~ ., data = train,
  metric = "ROC",
  method = "glmnet",
  trControl = myControl
)

 cvfit <- cv.glmnet(cvtrain.x, cvtrain.y,  metric = "ROC", method = "glmnet", trControl = myControl) 
 fit.pred <- predict(cvfit, newx = cvtest.x, type = "response")
 pred <- prediction(fit.pred[,1], cvtest.y)
 roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
 auc.train <- performance(pred, measure = "auc")
 auc.train <- auc.train@y.values
 
 cv.aucs[i]<-auc.train[[1]]
}

hist(cv.aucs)
summary(cv.aucs)








#<<<need to use imputed data>>>   Train/test set split and compare knn,regular regression,
#and a regression tree's test MSE.
library(tree)
set.seed(123)
#index<-sample(1:200,100)
train<-heartData[1:3000,]
test<-heartData[3001:nrow(heartData),]

train.tree<-tree(TenYearCHD~age+sysBP+cigsPerDay+male+diabetes+prevalentStroke,train)
summary(train.tree)
plot(train.tree)
text(train.tree,pretty=0)


plot(cv.tree(train.tree,FUN=prune.tree,method="deviance"))

testMSE<-mean((test$sales- predict(train.tree,newdata=test) )^2)


knn<-FNN::knn.reg(train = train[,-c(3,4)], test =test[,-c(3,4)], y = train$TenYearCHD, k = 5)
testMSE.knn<-mean( ( test$TenYearCHD-knn$pred)^2)

#full.fit<-lm(sales~TV+radio+TV:radio,train)
#testMSE.ols<-mean((test$sales-predict(full.fit,test))^2)


testMSE
testMSE.knn
#testMSE.ols












